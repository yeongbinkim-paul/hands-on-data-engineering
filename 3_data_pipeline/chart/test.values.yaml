env: "test"

scheduler:
  resources:
    requests:
      cpu: 1000m
      memory: 1Gi
    limits:
      cpu: 1500m
      memory: 2Gi

triggerer:
  enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 1Gi
    limits:
      cpu: 100m
      memory: 1Gi

workers:
  enabled: true
  replica: 4
  resources:
    requests:
      cpu: 1000m
      memory: 2Gi
    limits:
      cpu: 1000m
      memory: 2Gi

webserver:
  resources:
    requests:
      cpu: 250m
      memory: 1Gi
    limits:
      cpu: 250m
      memory: 1Gi

gitSync:
  enabled: true
  repo: https://github.com/yeongbinkim-paul/hands-on-data-engineering
  branch: dev
  subPath: 3_data_pipeline/dags
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 100m
      memory: 128Mi

postgres:
  enabled: true
  postgresPassword: "test"
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

airflowCfg: |-
  [celery]
  worker_concurrency = 16
  worker_autoscale = 24,16
  operation_timeout = 3.0
  flower_basic_auth = admin:admin

  [celery_kubernetes_executor]
  kubernetes_queue = kubernetes

  [core]
  min_serialized_dag_update_interval = 30
  min_serialized_dag_fetch_interval = 10
  max_num_rendered_ti_fields_per_task = 30

  colored_console_log = False
  dags_folder = /opt/airflow/dags
  executor = CeleryKubernetesExecutor
  load_examples = False
  parallelism = 10000
  dagbag_import_timeout = 300
  dag_file_processor_timeout = 300
  max_active_tasks_per_dag = 2000
  max_active_runs_per_dag = 128

  [database]
  sql_alchemy_max_overflow = -1
  sql_alchemy_pool_size = 0

  [elasticsearch]
  json_format = True
  log_id_template = {dag_id}_{task_id}_{execution_date}_{try_number}

  [elasticsearch_configs]
  max_retries = 3
  retry_timeout = True
  timeout = 30

  [kerberos]
  ccache = /var/kerberos-ccache/cache
  keytab = /etc/airflow.keytab
  principal = airflow@FOO.COM
  reinit_frequency = 3600

  [kubernetes]
  airflow_configmap = {{ .Chart.Name }}-airflow-config
  airflow_local_settings_configmap = {{ .Chart.Name }}-airflow-config
  multi_namespace_mode = False

  namespace = {{ .Release.Namespace }}
  worker_container_repository = {{ .Values.images.airflow.repository }}
  worker_container_tag = {{ .Values.images.airflow.tag }}
  delete_worker_pods_on_failure = False # For debugging
  pod_template_file = /opt/airflow/pod_templates/pod_template_file.yaml
  delete_worker_pods = True
  worker_pods_creation_batch_size = 2000
  worker_pods_queued_check_interval = 2
  worker_pods_pending_timeout = 100
  worker_pods_pending_timeout_check_interval = 30

  [logging]
  colored_console_log = True

  [metrics]
  statsd_host = test-airflow-statsd
  statsd_on = True
  statsd_port = 9126
  statsd_prefix = airflow

  [scheduler]
  standalone_dag_processor = False
  scheduler_zombie_task_threshold = 100
  schedule_after_task_execution = True
  run_duration = 41460
  scheduler_heartbeat_sec = 2
  parsing_processes = 16
  scheduler_health_check_threshold = 200
  killed_task_cleanup_time = 10
  max_dagruns_per_loop_to_schedule = 2000
  statsd_host = test-airflow-statsd
  statsd_on = True
  statsd_port = 9126
  statsd_prefix = airflow

  [webserver]
  navbar_color =
  enable_proxy_fix = True
  rbac = True
  web_server_port = 8081

  [smart_sensor]
  use_smart_sensor = False

webserver_config.py: ~

airflowLocalSettings: |-
  from airflow.www.utils import UIAlert

  DASHBOARD_UIALERTS = [
      UIAlert(
        'Paul Data pipeline (Test) from hands on data engineering',
        category="error",
        roles=["Admin"],
        html=True,
      )
  ]

podTemplate: |
  apiVersion: v1
  kind: Pod
  metadata:
    name: dummy-name
    namespace: "{{ .Release.Namespace }}"
    labels:
      tier: airflow
      component: worker
      release: {{ .Chart.Name }}
  spec:
    containers:
      - args: []
        command: []
        envFrom:
          []
        env:
          - name: AIRFLOW__METRICS__STATSD_PORT
            value: "8125"
          - name: AIRFLOW__METRICS__STATSD_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: AIRFLOW__CORE__EXECUTOR
            value: LocalExecutor
          # Hard Coded Airflow Envs
        {{- include "standard_airflow_environment" . | nindent 6 }}
        image: {{ template "airflow_image" . }}
        imagePullPolicy: IfNotPresent
        name: base
        ports: []
        resources:
          {}
        volumeMounts:
          - mountPath: "/opt/airflow/logs"
            name: logs
          - name: config
            mountPath: "/opt/airflow/airflow.cfg"
            subPath: airflow.cfg
            readOnly: true
          - name: config
            mountPath: "/opt/airflow/config/airflow_local_settings.py"
            subPath: airflow_local_settings.py
            readOnly: true
          - name: dag-data
            mountPath: /opt/airflow/dags
            readOnly: true
    hostNetwork: false
    restartPolicy: Never
    securityContext:
      runAsUser: 50000
      fsGroup: 0
    nodeSelector:
      {}
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                component: worker
            topologyKey: kubernetes.io/hostname
          weight: 100
    tolerations:
      []
    serviceAccountName: {{ .Chart.Name }}
    volumes:
    - emptyDir: {}
      name: logs
    - configMap:
        name: airflow-config-{{ .Values.env }}
      name: config
    - name: dag-data
      persistentVolumeClaim:
        claimName: "dag-syncer-pvc-{{ .Values.env }}"
